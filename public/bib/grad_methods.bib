@incollection{bowerExperimentalMethodsCognitive1989,
  title = {Experimental Methods in Cognitive Science},
  booktitle = {Foundations of Cognitive Science},
  author = {Bower, Gordon H. and Clapper, John P.},
  date = {1989},
  pages = {245--300},
  publisher = {The MIT Press},
  location = {Cambridge, MA, US},
  abstract = {naturalistic observation / correlational studies / controlled experiments / measuring experimental effects / isolating causal effects / coordinating theory with observables / experiments on human cognitive processes / characterizing psychological processes / analyzing representational types / additive factors method /dual tasks / signal detection theory  argued that intuitive instrospection is often a weak, uninformative, even biased, measuring instrument for rapid, nonconscious cognitive processes  we presented a few general methods used in experimental studies of cognition and indicated how analogical theories can be placed in correspondence to observable, experimental events / we then reviewed some specific techniques used in studies of memory and language processing, trying in most cases to indicate a few substantive issues addressed by them (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  isbn = {978-0-262-16112-1 978-0-262-66086-0},
  keywords = {Cognitive Processes,Experimental Methods,Language,Learning,Memory},
  file = {/Users/glupyan/Zotero/storage/YP72FICL/1990-97026-007.html}
}

@article{kayWhyYouShouldnt2025,
  title = {Why You Shouldn’t Trust Data Collected on {{MTurk}}},
  author = {Kay, Cameron S.},
  date = {2025-11-10},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {57},
  number = {12},
  pages = {340},
  issn = {1554-3528},
  doi = {10.3758/s13428-025-02852-7},
  url = {https://doi.org/10.3758/s13428-025-02852-7},
  urldate = {2026-01-12},
  abstract = {Several prior studies have used advanced methodological techniques to demonstrate that there is an issue with the quality of data that can be collected on Amazon’s Mechanical Turk (MTurk). The goal of the present project was to provide an accessible demonstration of this issue. We administered 27 semantic antonyms—pairs of items that assess clearly contradictory content (e.g., “I talk a lot” and “I rarely talk”)—to samples drawn from Connect (N1\,=\,100), Prolific (N2\,=\,100), and MTurk (N3\,=\,400; N4\,=\,600). Despite most of these item pairs being negatively correlated on Connect and Prolific, over 96\% were positively correlated on MTurk. This issue could not be remedied by screening the data using common attention check measures nor by recruiting only “high-productivity” and “high-reputation” participants. These findings provide clear evidence that data collected on MTurk simply cannot be trusted.},
  langid = {english},
  keywords = {Amazon’s Mechanical Turk,Careless responding,Data quality,Invalid responding,Psychometrics,Survey design and methodology},
  file = {/Users/glupyan/Zotero/storage/KJ3CZEJR/Kay - 2025 - Why you shouldn’t trust data collected on MTurk.pdf}
}


@article{chiangCatchingCrumbsTable2000,
  title = {Catching Crumbs from the Table},
  author = {Chiang, Ted},
  date = {2000-06},
  journaltitle = {Nature},
  volume = {405},
  number = {6786},
  pages = {517--517},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/35014679},
  url = {https://www.nature.com/articles/35014679},
  urldate = {2026-01-15},
  abstract = {In the face of metahuman science, humans have become metascientists.},
  langid = {english},
  keywords = {Humanities and Social Sciences,multidisciplinary,Science},
  file = {/Users/glupyan/Zotero/storage/NJLAZCEJ/Chiang - 2000 - Catching crumbs from the table.pdf}
}

@article{charlesworthExtractingIntersectionalStereotypes2024,
  title = {Extracting Intersectional Stereotypes from Embeddings: {{Developing}} and Validating the {{Flexible Intersectional Stereotype Extraction}} Procedure},
  shorttitle = {Extracting Intersectional Stereotypes from Embeddings},
  author = {Charlesworth, Tessa E S and Ghate, Kshitish and Caliskan, Aylin and Banaji, Mahzarin R},
  date = {2024-03-01},
  journaltitle = {PNAS Nexus},
  shortjournal = {PNAS Nexus},
  volume = {3},
  number = {3},
  pages = {pgae089},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae089},
  url = {https://doi.org/10.1093/pnasnexus/pgae089},
  urldate = {2026-01-12},
  abstract = {Social group–based identities intersect. The meaning of “woman” is modulated by adding social class as in “rich woman” or “poor woman.” How does such intersectionality operate at-scale in everyday language? Which intersections dominate (are most frequent)? What qualities (positivity, competence, warmth) are ascribed to each intersection? In this study, we make it possible to address such questions by developing a stepwise procedure, Flexible Intersectional Stereotype Extraction (FISE), applied to word embeddings (GloVe; BERT) trained on billions of words of English Internet text, revealing insights into intersectional stereotypes. First, applying FISE to occupation stereotypes across intersections of gender, race, and class showed alignment with ground-truth data on occupation demographics, providing initial validation. Second, applying FISE to trait adjectives showed strong androcentrism (Men) and ethnocentrism (White) in dominating everyday English language (e.g. White + Men are associated with 59\% of traits; Black + Women with 5\%). Associated traits also revealed intersectional differences: advantaged intersectional groups, especially intersections involving Rich, had more common, positive, warm, competent, and dominant trait associates. Together, the empirical insights from FISE illustrate its utility for transparently and efficiently quantifying intersectional stereotypes in existing large text corpora, with potential to expand intersectionality research across unprecedented time and place. This project further sets up the infrastructure necessary to pursue new research on the emergent properties of intersectional identities.},
  file = {/Users/glupyan/Zotero/storage/EDP8P7AE/Charlesworth et al. - 2024 - Extracting intersectional stereotypes from embeddings Developing and validating the Flexible Inters.pdf;/Users/glupyan/Zotero/storage/XUQZJ8X8/pgae089.html}
}

@book{clarkConstructingValidityBasic2016,
  title = {Constructing Validity: {{Basic}} Issues in Objective Scale Development},
  shorttitle = {Constructing Validity},
  author = {Clark, Lee Anna and Watson, David},
  date = {2016},
  series = {Methodological Issues and Strategies in Clinical Research, 4th Ed},
  pages = {203},
  publisher = {American Psychological Association},
  location = {Washington, DC, US},
  doi = {10.1037/14805-012},
  abstract = {This reprinted article originally appeared in Psychological Assessment, 1995 (Sep), Vol 7(3), 309-319. (The following abstract of the original article appeared in record 1996-93318-001.) A primary goal of scale development is to create a valid measure of an underlying construct. We discuss theoretical principles, practical issues, and pragmatic decisions to help developers maximize the construct validity of scales and subscales. First, it is essential to begin with a clear conceptualization of the target construct. Moreover, the content of the initial item pool should be overinclusive and item wording needs careful attention. Next, the item pool should be tested, along with variables that assess closely related constructs, on a heterogeneous sample representing the entire range of the target population. Finally, in selecting scale items, the goal is unidimensionality rather than internal consistency; this means that virtually all interitem correlations should be moderate in magnitude. Factor analysis can play a crucial role in ensuring the unidimensionality and discriminant validity of scales. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  isbn = {978-1-4338-2091-5 978-1-4338-2140-0 978-1-4338-2092-2},
  pagetotal = {187},
  keywords = {Construct Validity,Item Analysis (Test),Statistical Validity,Test Construction},
  file = {/Users/glupyan/Zotero/storage/UCS9SN5Z/2015-32022-012.html}
}

@article{reiseItemResponseTheory2009,
  title = {Item {{Response Theory}} and {{Clinical Measurement}}},
  author = {Reise, Steven P. and Waller, Niels G.},
  date = {2009-04-27},
  journaltitle = {Annual Review of Clinical Psychology},
  volume = {5},
  pages = {27--48},
  publisher = {Annual Reviews},
  issn = {1548-5943, 1548-5951},
  doi = {10.1146/annurev.clinpsy.032408.153553},
  url = {https://www.annualreviews.org/content/journals/10.1146/annurev.clinpsy.032408.153553},
  urldate = {2026-01-12},
  abstract = {In this review, we examine studies that use item response theory (IRT) to explore the psychometric properties of clinical measures. Next, we consider how IRT has been used in clinical research for: scale linking, computerized adaptive testing, and differential item functioning analysis. Finally, we consider the scale properties of IRT trait scores. We conclude that there are notable differences between cognitive and clinical measures that have relevance for IRT modeling. Future research should be directed toward a better understanding of the metric of the latent trait and the psychological processes that lead to individual differences in item response behaviors.},
  issue = {Volume 5, 2009},
  langid = {english},
  file = {/Users/glupyan/Zotero/storage/EDL4FI6G/annurev.clinpsy.032408.html}
}



@article{andersHowOnlineStudies2026,
  title = {How Online Studies Must Increase Their Defences against {{AI}}},
  author = {Anders, Gerrit and Buder, Jürgen and Papenmeier, Frank and Huff, Markus},
  date = {2026-01-08},
  journaltitle = {Communications Psychology},
  shortjournal = {Commun Psychol},
  publisher = {Nature Publishing Group},
  issn = {2731-9121},
  doi = {10.1038/s44271-025-00388-2},
  url = {https://www.nature.com/articles/s44271-025-00388-2},
  urldate = {2026-01-15},
  abstract = {LLM agents can now pass as human participants, threatening the validity of online social science. We urge a shift from ad-hoc checks to multi-layered, adaptive defenses, borrowing from internet anti-bot practice, and call for cooperation across researchers, platforms, and institutions, to guard against this challenge.},
  langid = {english},
  keywords = {Psychology,Sociology}
}


@article{clarkTestretestReliabilityCommon2022,
  title = {Test-Retest Reliability for Common Tasks in Vision Science},
  author = {Clark, Kait and Birch-Hurst, Kayley and Pennington, Charlotte R. and Petrie, Austin C. P. and Lee, Joshua T. and Hedge, Craig},
  date = {2022-07-11},
  journaltitle = {Journal of Vision},
  shortjournal = {J Vis},
  volume = {22},
  number = {8},
  eprint = {35904797},
  eprinttype = {pubmed},
  pages = {18},
  issn = {1534-7362},
  doi = {10.1167/jov.22.8.18},
  abstract = {Research in perception and attention has typically sought to evaluate cognitive mechanisms according to the average response to a manipulation. Recently, there has been a shift toward appreciating the value of individual differences and the insight gained by exploring the impacts of between-participant variation on human cognition. However, a recent study suggests that many robust, well-established cognitive control tasks suffer from surprisingly low levels of test-retest reliability (Hedge, Powell, \& Sumner, 2018b). We tested a large sample of undergraduate students (n = 160) in two sessions (separated by 1-3 weeks) on four commonly used tasks in vision science. We implemented measures that spanned a range of perceptual and attentional processes, including motion coherence (MoCo), useful field of view (UFOV), multiple-object tracking (MOT), and visual working memory (VWM). Intraclass correlations ranged from good to poor, suggesting that some task measures are more suitable for assessing individual differences than others. VWM capacity (intraclass correlation coefficient [ICC] = 0.77), MoCo threshold (ICC = 0.60), UFOV middle accuracy (ICC = 0.60), and UFOV outer accuracy (ICC = 0.74) showed good-to-excellent reliability. Other measures, namely the maximum number of items tracked in MOT (ICC = 0.41) and UFOV number accuracy (ICC = 0.48), showed moderate reliability; the MOT threshold (ICC = 0.36) and UFOV inner accuracy (ICC = 0.30) showed poor reliability. In this paper, we present these results alongside a summary of reliabilities estimated previously for other vision science tasks. We then offer useful recommendations for evaluating test-retest reliability when considering a task for use in evaluating individual differences.},
  langid = {english},
  pmcid = {PMC9344221},
  keywords = {Attention,Cognition,Humans,Memory Short-Term,Reproducibility of Results,Vision Ocular},
  file = {/Users/glupyan/Zotero/storage/ADHFR4WP/Clark et al. - 2022 - Test-retest reliability for common tasks in vision science.pdf}
}

@article{cohenEarthP,
  title = {The Earth Is Round (P<.05)},
  author = {Cohen, Jacob},
  date = {1994},
  journaltitle = {American Psychologist},
  volume = {49},
  number = {12},
  pages = {997--1003},
  issn = {1935-990X(Electronic),0003-066X(Print)},
  doi = {10.1037/0003-066X.49.12.997},
  abstract = {After 4 decades of severe criticism, the ritual of null hypothesis significance testing (mechanical dichotomous decisions around a sacred .05 criterion) still persists. This article reviews the problems with this practice, including near universal misinterpretation of p as the probability that H₀ is false, the misinterpretation that its complement is the probability of successful replication, and the mistaken assumption that if one rejects H₀ one thereby affirms the theory that led to the test. Exploratory data analysis and the use of graphic methods, a steady improvement in and a movement toward standardization in measurement, an emphasis on estimating effect sizes using confidence intervals, and the informed use of available statistical methods are suggested. For generalization, psychologists must finally rely, as has been done in all the older sciences, on replication. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Null Hypothesis Testing},
  file = {/Users/glupyan/Zotero/storage/QQKVVK2R/1995-12080-001.html}
}

@article{corneilleSelfreportsAreBetter2024b,
  title = {Self-Reports Are Better Measurement Instruments than Implicit Measures},
  author = {Corneille, Olivier and Gawronski, Bertram},
  date = {2024-12},
  journaltitle = {Nature Reviews Psychology},
  shortjournal = {Nat Rev Psychol},
  volume = {3},
  number = {12},
  pages = {835--846},
  publisher = {Nature Publishing Group},
  issn = {2731-0574},
  doi = {10.1038/s44159-024-00376-z},
  url = {https://www.nature.com/articles/s44159-024-00376-z},
  urldate = {2026-01-05},
  abstract = {Self-report measures directly ask respondents to report their mental content, such as thoughts and feelings. By contrast, implicit measures aim to assess thoughts and feelings using performance indicators (for example, response times, error rates and response frequencies) under conditions that favour automatic processing. Implicit measures are now widely used in psychological science and beyond, because they are assumed to be superior to self-reports in various ways. In this Perspective, we argue that, despite the enthusiasm for implicit measures, self-reports are most often the better measurement option. First, the use of implicit measures is often based on mistaken assumptions about the disadvantages of self-reports. Second, self-reports have advantageous characteristics that are currently unmatched in implicit measures. We call for a more sophisticated use of self-reports and for caution when using implicit measures in basic and applied research.},
  langid = {english},
  keywords = {Human behaviour,Psychology}
}

@article{cornsweetStaircaseMethodPsychophysics1962,
  title = {The {{Staircase-Method}} in {{Psychophysics}}},
  author = {Cornsweet, Tom N.},
  date = {1962},
  journaltitle = {The American Journal of Psychology},
  volume = {75},
  number = {3},
  eprint = {1419876},
  eprinttype = {jstor},
  pages = {485--491},
  publisher = {University of Illinois Press},
  issn = {0002-9556},
  doi = {10.2307/1419876},
  url = {https://www.jstor.org/stable/1419876},
  urldate = {2026-01-05}
}

@article{cortinaAlphaOmegaLook2020,
  title = {From Alpha to Omega and beyond! {{A}} Look at the Past, Present, and (Possible) Future of Psychometric Soundness in the {{Journal}} of {{Applied Psychology}}},
  author = {Cortina, Jose M. and Sheng, Zitong and Keener, Sheila K. and Keeler, Kathleen R. and Grubb, Leah K. and Schmitt, Neal and Tonidandel, Scott and Summerville, Karoline M. and Heggestad, Eric D. and Banks, George C.},
  date = {2020},
  journaltitle = {Journal of Applied Psychology},
  volume = {105},
  number = {12},
  pages = {1351--1381},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1854},
  doi = {10.1037/apl0000815},
  abstract = {The psychometric soundness of measures has been a central concern of articles published in the Journal of Applied Psychology (JAP) since the inception of the journal. At the same time, it isn’t clear that investigators and reviewers prioritize psychometric soundness to a degree that would allow one to have sufficient confidence in conclusions regarding constructs. The purposes of the present article are to (a) examine current scale development and evaluation practices in JAP; (b) compare these practices to recommended practices, previous practices, and practices in other journals; and (c) use these comparisons to make recommendations for reviewers, editors, and investigators regarding the creation and evaluation of measures including Excel-based calculators for various indices. Finally, given that model complexity appears to have increased the need for short scales, we offer a user-friendly R Shiny app (https://orgscience.uncc.edu/about-us/resources) that identifies the subset of items that maximize a variety of psychometric criteria rather than merely maximizing alpha. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Applied Psychology,Industrial and Organizational Psychology,Psychometrics,Scientific Communication,Test Forms,Test Reliability,Test Validity},
  file = {/Users/glupyan/Zotero/storage/FMU7S96G/2020-58909-001.html}
}

@article{cortinaWhatCoefficientAlpha1993,
  title = {What Is Coefficient Alpha? {{An}} Examination of Theory and Applications},
  shorttitle = {What Is Coefficient Alpha?},
  author = {Cortina, Jose M.},
  date = {1993},
  journaltitle = {Journal of Applied Psychology},
  volume = {78},
  number = {1},
  pages = {98--104},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1939-1854},
  doi = {10.1037/0021-9010.78.1.98},
  abstract = {Psychological research involving scale construction has been hindered considerably by a widespread lack of understanding of coefficient alpha and reliability theory in general. A discussion of the assumptions and meaning of coefficient alpha is presented. This discussion is followed by a demonstration of the effects of test length and dimensionality on alpha by calculating the statistic for hypothetical tests with varying numbers of items, numbers of orthogonal dimensions, and average item intercorrelations. Recommendations for the proper use of coefficient alpha are offered. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Statistical Analysis,Test Construction,Test Forms,Test Reliability},
  file = {/Users/glupyan/Zotero/storage/GYWDCFB9/1993-19965-001.html}
}

@article{cronbachTwoDisciplinesScientific1957,
  title = {The Two Disciplines of Scientific Psychology},
  author = {Cronbach, Lee J.},
  date = {1957},
  journaltitle = {American Psychologist},
  volume = {12},
  pages = {671--684},
  publisher = {American Psychological Association},
  location = {US},
  issn = {1935-990X},
  doi = {10.1037/h0043943},
  abstract = {"No man can be acquainted with all of psychology today." The past and future place within psychology of 2 historic streams of method, thought, and affiliation—experimental psychology and correlational psychology—is discussed in this address of the President at the 65th annual convention of the APA. "The well-known virtue of the experimental method is that it brings situational variables under tight control… . The correlation method, for its part, can study what man has not learned to control or can never hope to control… . A true federation of the disciplines is required. Kept independent, they can give only wrong answers or no answers at all regarding certain important problems… . Correlational psychology studies only variance among organisms; experimental psychology studies only variance among treatments. A united discipline will study both of these, but it will also be concerned with the otherwise neglected interactions between organismic and treatment variables. Our job is to invent constructs and to form a network of laws which permits prediction." (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
  keywords = {Experimental Psychology,History of Psychology},
  file = {/Users/glupyan/Zotero/storage/M3IEAL4V/1959-02450-001.html}
}

@article{enkaviLargescaleAnalysisTest2019a,
  title = {Large-Scale Analysis of Test–Retest Reliabilities of Self-Regulation Measures},
  author = {Enkavi, A. Zeynep and Eisenberg, Ian W. and Bissett, Patrick G. and Mazza, Gina L. and MacKinnon, David P. and Marsch, Lisa A. and Poldrack, Russell A.},
  date = {2019-03-19},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {116},
  number = {12},
  pages = {5472--5477},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.1818430116},
  url = {https://www.pnas.org/doi/10.1073/pnas.1818430116},
  urldate = {2026-01-05},
  abstract = {The ability to regulate behavior in service of long-term goals is a widely studied psychological construct known as self-regulation. This wide interest is in part due to the putative relations between self-regulation and a range of real-world behaviors. Self-regulation is generally viewed as a trait, and individual differences are quantified using a diverse set of measures, including self-report surveys and behavioral tasks. Accurate characterization of individual differences requires measurement reliability, a property frequently characterized in self-report surveys, but rarely assessed in behavioral tasks. We remedy this gap by (i) providing a comprehensive literature review on an extensive set of self-regulation measures and (ii) empirically evaluating test–retest reliability of this battery in a new sample. We find that dependent variables (DVs) from self-report surveys of self-regulation have high test–retest reliability, while DVs derived from behavioral tasks do not. This holds both in the literature and in our sample, although the test–retest reliability estimates in the literature are highly variable. We confirm that this is due to differences in between-subject variability. We also compare different types of task DVs (e.g., model parameters vs. raw response times) in their suitability as individual difference DVs, finding that certain model parameters are as stable as raw DVs. Our results provide greater psychometric footing for the study of self-regulation and provide guidance for future studies of individual differences in this domain.},
  file = {/Users/glupyan/Zotero/storage/IVMR7A7D/Enkavi et al. - 2019 - Large-scale analysis of test–retest reliabilities of self-regulation measures.pdf}
}

@article{feynmanCargoCultScience1974,
  title = {Cargo Cult Science},
  author = {Feynman, Richard P.},
  date = {1974},
  journaltitle = {Engineering and Science},
  volume = {37},
  number = {7},
  pages = {10--13},
  file = {/Users/glupyan/Zotero/storage/ALER8FYL/Feynman - 1974 - Cargo cult science.pdf}
}

@article{forscherChaosBrickyard1963,
  title = {Chaos in the {{Brickyard}}},
  author = {Forscher, Bernard K.},
  date = {1963-10-18},
  journaltitle = {Science},
  volume = {142},
  number = {3590},
  pages = {339--339},
  publisher = {American Association for the Advancement of Science},
  doi = {10.1126/science.142.3590.339.a},
  url = {https://www.science.org/doi/10.1126/science.142.3590.339.a},
  urldate = {2026-01-05}
}

@article{funderEvaluatingEffectSize2019,
  title = {Evaluating {{Effect Size}} in {{Psychological Research}}: {{Sense}} and {{Nonsense}}},
  shorttitle = {Evaluating {{Effect Size}} in {{Psychological Research}}},
  author = {Funder, David C. and Ozer, Daniel J.},
  date = {2019-06-01},
  journaltitle = {Advances in Methods and Practices in Psychological Science},
  volume = {2},
  number = {2},
  pages = {156--168},
  publisher = {SAGE Publications Inc},
  issn = {2515-2459},
  doi = {10.1177/2515245919847202},
  url = {https://doi.org/10.1177/2515245919847202},
  urldate = {2025-11-03},
  abstract = {Effect sizes are underappreciated and often misinterpreted—the most common mistakes being to describe them in ways that are uninformative (e.g., using arbitrary standards) or misleading (e.g., squaring effect-size rs). We propose that effect sizes can be usefully evaluated by comparing them with well-understood benchmarks or by considering them in terms of concrete consequences. In that light, we conclude that when reliably estimated (a critical consideration), an effect-size r of .05 indicates an effect that is very small for the explanation of single events but potentially consequential in the not-very-long run, an effect-size r of .10 indicates an effect that is still small at the level of single events but potentially more ultimately consequential, an effect-size r of .20 indicates a medium effect that is of some explanatory and practical use even in the short run and therefore even more important, and an effect-size r of .30 indicates a large effect that is potentially powerful in both the short and the long run. A very large effect size (r = .40 or greater) in the context of psychological research is likely to be a gross overestimate that will rarely be found in a large sample or in a replication. Our goal is to help advance the treatment of effect sizes so that rather than being numbers that are ignored, reported without interpretation, or interpreted superficially or incorrectly, they become aspects of research reports that can better inform the application and theoretical development of psychological research.},
  langid = {english},
  file = {/Users/glupyan/Zotero/storage/LBQG3XGJ/Funder and Ozer - 2019 - Evaluating Effect Size in Psychological Research Sense and Nonsense.pdf}
}

@article{gelmanInterrogatingCargoCult2025,
  title = {Interrogating the “Cargo Cult Science” Metaphor},
  author = {Gelman, Andrew and Higgs, Megan},
  date = {2025-04-01},
  journaltitle = {Theory and Society},
  shortjournal = {Theor Soc},
  volume = {54},
  number = {2},
  pages = {197--207},
  issn = {1573-7853},
  doi = {10.1007/s11186-025-09614-6},
  url = {https://doi.org/10.1007/s11186-025-09614-6},
  urldate = {2026-01-05},
  abstract = {Over the past fifty years, the term “cargo cult” has been used to describe the actions of scientists who appear to follow forms of scientific inquiry but without the understanding and self-criticism that are essential to real scientific progress. The term has served a useful role by providing a short and catchy label to something that is otherwise difficult to explain. However, the term is also fraught with historical and cultural baggage and, in our opinion, encourages crossing of a subtle line between criticizing discipline methodological norms and criticizing the individuals currently carrying out those norms as part of a complex and context dependent social process. We find that carefully interrogating the term itself holds some important lessons for improvement in the science reform movement.},
  langid = {english},
  keywords = {Ritual science,Science reform,Sociology of science,Statistics},
  file = {/Users/glupyan/Zotero/storage/U7TRJHXV/Gelman and Higgs - 2025 - Interrogating the “cargo cult science” metaphor.pdf}
}

@article{giner-sorollaCrisisEvidenceCrisis2019,
  title = {From Crisis of Evidence to a "Crisis" of Relevance? {{Incentive-based}} Answers for Social Psychology’s Perennial Relevance Worries},
  shorttitle = {From Crisis of Evidence to a "Crisis" of Relevance?},
  author = {Giner-Sorolla, Roger},
  date = {2019},
  journaltitle = {European Review of Social Psychology},
  volume = {30},
  number = {1},
  pages = {1--38},
  publisher = {Taylor \& Francis},
  location = {United Kingdom},
  issn = {1479-277X},
  doi = {10.1080/10463283.2018.1542902},
  abstract = {Current controversies in social psychology have sparked the promotion of new rules for evidence in the field. This “crisis of evidence” echoes prior concerns from the 1970s about a so-called “crisis of social psychology”, with such issues as replication and statistical significance once more under examination. I argue that parallel concerns about the relevance of our research, raised but not completely resolved in the 1970s crisis, also deserve a fresh look. In particular, the advances made in the current crisis of evidence came about because of changes in academic career incentives, particularly publishing. Today, many voices in psychology urge greater respect for relevance in topics, methods and communication, but the lack of clear and concrete incentives to do so has stood in the way of answers. I diagnose the current incentive structures, propose partial solutions that are within the reach of journal editors and professional societies, and conclude by discussing the links between relevance and evidence, as well as special challenges to the relevance of social psychology post-2016. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  keywords = {Communication,Incentives,Social Psychology},
  file = {/Users/glupyan/Zotero/storage/YGVDNAIZ/Giner-Sorolla - 2019 - From crisis of evidence to a crisis of relevance Incentive-based answers for social psychology’s.pdf;/Users/glupyan/Zotero/storage/9J363EQL/2018-57565-001.html}
}

@article{goldstone_discovering_2016,
  title = {Discovering {{Psychological Principles}} by {{Mining Naturally Occurring Data Sets}}},
  author = {Goldstone, Robert L. and Lupyan, G.},
  date = {2016-07},
  journaltitle = {Topics in Cognitive Science},
  volume = {8},
  number = {3},
  pages = {548--568},
  issn = {17568757},
  doi = {10.1111/tops.12212},
  url = {http://doi.wiley.com/10.1111/tops.12212},
  urldate = {2016-09-06},
  langid = {english}
}

@article{hedgeReliabilityParadoxWhy2018,
  title = {The Reliability Paradox: {{Why}} Robust Cognitive Tasks Do Not Produce Reliable Individual Differences},
  shorttitle = {The Reliability Paradox},
  author = {Hedge, Craig and Powell, Georgina and Sumner, Petroc},
  date = {2018-06-01},
  journaltitle = {Behavior Research Methods},
  shortjournal = {Behav Res},
  volume = {50},
  number = {3},
  pages = {1166--1186},
  issn = {1554-3528},
  doi = {10.3758/s13428-017-0935-1},
  url = {https://doi.org/10.3758/s13428-017-0935-1},
  urldate = {2024-01-09},
  abstract = {Individual differences in cognitive paradigms are increasingly employed to relate cognition to brain structure, chemistry, and function. However, such efforts are often unfruitful, even with the most well established tasks. Here we offer an explanation for failures in the application of robust cognitive paradigms to the study of individual differences. Experimental effects become well established – and thus those tasks become popular – when between-subject variability is low. However, low between-subject variability causes low reliability for individual differences, destroying replicable correlations with other factors and potentially undermining published conclusions drawn from correlational relationships. Though these statistical issues have a long history in psychology, they are widely overlooked in cognitive psychology and neuroscience today. In three studies, we assessed test-retest reliability of seven classic tasks: Eriksen Flanker, Stroop, stop-signal, go/no-go, Posner cueing, Navon, and Spatial-Numerical Association of Response Code (SNARC). Reliabilities ranged from 0 to .82, being surprisingly low for most tasks given their common use. As we predicted, this emerged from low variance between individuals rather than high measurement variance. In other words, the very reason such tasks produce robust and easily replicable experimental effects – low between-participant variability – makes their use as correlational tools problematic. We demonstrate that taking such reliability estimates into account has the potential to qualitatively change theoretical conclusions. The implications of our findings are that well-established approaches in experimental psychology and neuropsychology may not directly translate to the study of individual differences in brain structure, chemistry, and function, and alternative metrics may be required.},
  langid = {english},
  keywords = {Difference scores,Individual differences,Reaction time,Reliability,Response control},
  file = {/Users/glupyan/Zotero/storage/BLDA4KTS/Hedge et al. - 2018 - The reliability paradox Why robust cognitive task.pdf}
}

@article{leekAdaptiveProceduresPsychophysical2001,
  title = {Adaptive Procedures in Psychophysical Research},
  author = {Leek, Marjorie R.},
  date = {2001-11-01},
  journaltitle = {Perception \& Psychophysics},
  shortjournal = {Perception \& Psychophysics},
  volume = {63},
  number = {8},
  pages = {1279--1292},
  issn = {1532-5962},
  doi = {10.3758/BF03194543},
  url = {https://doi.org/10.3758/BF03194543},
  urldate = {2026-01-05},
  abstract = {As research on sensation and perception has grown more sophisticated during the last century, new adaptive methodologies have been developed to increase efficiency and reliability of measurement. An experimental procedure is said to be adaptive if the physical characteristics of the stimuli on each trial are determined by the stimuli and responses that occurred in the previous trial or sequence of trials. In this paper, the general development of adaptive procedures is described, and three commonly used methods are reviewed. Typically, a threshold value is measured using these methods, and, in some cases, other characteristics of the psychometric function underlying perceptual performance, such as slope, may be developed. Results of simulations and experiments with human subjects are reviewed to evaluate the utility of these adaptive procedures and the special circumstances under which one might be superior to another.},
  langid = {english},
  keywords = {Acoustical Society,Adaptive Procedure,Psychometric Function,Staircase Procedure,Stimulus Level},
  file = {/Users/glupyan/Zotero/storage/F75GB8XV/Leek - 2001 - Adaptive procedures in psychophysical research.pdf}
}

@article{lovakovEmpiricallyDerivedGuidelines2021,
  title = {Empirically Derived Guidelines for Effect Size Interpretation in Social Psychology},
  author = {Lovakov, Andrey and Agadullina, Elena R.},
  date = {2021},
  journaltitle = {European Journal of Social Psychology},
  volume = {51},
  number = {3},
  pages = {485--504},
  issn = {1099-0992},
  doi = {10.1002/ejsp.2752},
  url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/ejsp.2752},
  urldate = {2026-01-05},
  abstract = {This study estimates empirically derived guidelines for effect size interpretation for research in social psychology overall and sub-disciplines within social psychology, based on analysis of the true distributions of the two types of effect size measures widely used in social psychology (correlation coefficient and standardized mean differences). Analysis of empirically derived distributions of 12,170 correlation coefficients and 6,447 Cohen's d statistics extracted from studies included in 134 published meta-analyses revealed that the 25th, 50th, and 75th percentiles corresponded to correlation coefficient values of 0.12, 0.24, and 0.41 and to Cohen's d values of 0.15, 0.36, and 0.65 respectively. The analysis suggests that the widely used Cohen's guidelines tend to overestimate medium and large effect sizes. Empirically derived effect size distributions in social psychology overall and its sub-disciplines can be used both for effect size interpretation and for sample size planning when other information about effect size is not available.},
  langid = {english},
  keywords = {Cohen's d,correlation,effect size,sample size},
  file = {/Users/glupyan/Zotero/storage/KQX36T9T/Lovakov and Agadullina - 2021 - Empirically derived guidelines for effect size interpretation in social psychology.pdf;/Users/glupyan/Zotero/storage/V2UK6Z6Q/ejsp.html}
}

@book{macmillanDetectionTheoryUsers2005,
  title = {Detection Theory: {{A}} User's Guide, 2nd Ed},
  shorttitle = {Detection Theory},
  author = {Macmillan, Neil A. and Creelman, C. Douglas},
  date = {2005},
  series = {Detection Theory: {{A}} User's Guide, 2nd Ed},
  pages = {xix, 492},
  publisher = {Lawrence Erlbaum Associates Publishers},
  location = {Mahwah, NJ, US},
  abstract = {Detection Theory, Second Edition is an introduction to one of the most important tools for analysis of data where choices must be made and performance is not perfect. Originally developed for evaluation of electronic detection, detection theory was adopted by psychologists as a way to understand sensory decision making, then embraced by students of human memory. It has since been utilized in areas as diverse as animal behavior and X-ray diagnosis. This book covers the basic principles of detection theory, with separate initial chapters on measuring detection and evaluating decision criteria. Some other features include (1) complete tools for application, including flowcharts, tables, pointers, and software; (2) student-friendly language; (3) complete coverage of content area, including both one-dimensional and multidimensional models; (4) separate, systematic coverage of sensitivity and response bias measurement; (5) integrated treatment of threshold and nonparametric approaches; (6) an organized, tutorial level introduction to multidimensional detection theory; (7) popular discrimination paradigms presented as applications of multidimensional detection theory: and (8) a new chapter on ideal observers and an updated chapter on adaptive threshold measurement. (PsycInfo Database Record (c) 2025 APA, all rights reserved)},
  isbn = {978-0-8058-4230-2 978-0-8058-4231-9},
  pagetotal = {xix, 492},
  keywords = {Auditory Perception,Auditory Stimulation,Experimental Design,Signal Detection (Perception),Visual Discrimination,Visual Perception,Visual Stimulation},
  file = {/Users/glupyan/Zotero/storage/2MRACFVC/2004-19022-000.html}
}

@article{marekReproducibleBrainwideAssociation2022,
  title = {Reproducible Brain-Wide Association Studies Require Thousands of Individuals},
  author = {Marek, Scott and Tervo-Clemmens, Brenden and Calabro, Finnegan J. and Montez, David F. and Kay, Benjamin P. and Hatoum, Alexander S. and Donohue, Meghan Rose and Foran, William and Miller, Ryland L. and Hendrickson, Timothy J. and Malone, Stephen M. and Kandala, Sridhar and Feczko, Eric and Miranda-Dominguez, Oscar and Graham, Alice M. and Earl, Eric A. and Perrone, Anders J. and Cordova, Michaela and Doyle, Olivia and Moore, Lucille A. and Conan, Gregory M. and Uriarte, Johnny and Snider, Kathy and Lynch, Benjamin J. and Wilgenbusch, James C. and Pengo, Thomas and Tam, Angela and Chen, Jianzhong and Newbold, Dillan J. and Zheng, Annie and Seider, Nicole A. and Van, Andrew N. and Metoki, Athanasia and Chauvin, Roselyne J. and Laumann, Timothy O. and Greene, Deanna J. and Petersen, Steven E. and Garavan, Hugh and Thompson, Wesley K. and Nichols, Thomas E. and Yeo, B. T. Thomas and Barch, Deanna M. and Luna, Beatriz and Fair, Damien A. and Dosenbach, Nico U. F.},
  date = {2022-03},
  journaltitle = {Nature},
  volume = {603},
  number = {7902},
  pages = {654--660},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-022-04492-9},
  url = {https://www.nature.com/articles/s41586-022-04492-9},
  urldate = {2026-01-05},
  abstract = {Magnetic resonance imaging (MRI) has transformed our understanding of the human brain through well-replicated mapping of abilities to specific structures (for example, lesion studies) and functions1–3 (for example, task functional MRI (fMRI)). Mental health research and care have yet to realize similar advances from MRI. A primary challenge has been replicating associations between inter-individual differences in brain structure~or~function and complex cognitive or mental health phenotypes (brain-wide association studies (BWAS)). Such BWAS have typically relied on sample sizes appropriate for classical brain mapping4 (the median neuroimaging study sample size is about 25), but potentially too small for capturing reproducible brain–behavioural phenotype associations5,6. Here we used three of the largest neuroimaging datasets currently available—with a total sample size of around 50,000 individuals—to quantify BWAS effect sizes and reproducibility as a function of sample size. BWAS associations were smaller than previously thought, resulting in statistically underpowered studies, inflated effect sizes and replication failures at typical sample sizes. As sample sizes grew into the thousands, replication rates began to improve and effect size inflation decreased. More robust BWAS effects were detected for functional MRI (versus structural), cognitive tests (versus mental health questionnaires) and multivariate methods (versus univariate). Smaller than expected brain–phenotype associations and variability across population subsamples can explain widespread BWAS replication failures. In contrast to non-BWAS approaches with larger effects (for example, lesions, interventions and within-person), BWAS reproducibility requires samples with thousands of individuals.},
  langid = {english},
  keywords = {Cognitive neuroscience,Psychology},
  file = {/Users/glupyan/Zotero/storage/AZU7Z4PC/Marek et al. - 2022 - Reproducible brain-wide association studies require thousands of individuals.pdf}
}

@article{meehlTheoryTestingPsychologyPhysics1967,
  title = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}: {{A Methodological Paradox}}},
  shorttitle = {Theory-{{Testing}} in {{Psychology}} and {{Physics}}},
  author = {Meehl, Paul E.},
  date = {1967-06-01},
  journaltitle = {Philosophy of Science},
  shortjournal = {Philosophy of Science},
  volume = {34},
  number = {2},
  pages = {103--115},
  issn = {0031-8248},
  doi = {10.1086/288135},
  url = {https://www.journals.uchicago.edu/doi/10.1086/288135},
  urldate = {2018-12-31},
  abstract = {Because physical theories typically predict numerical values, an improvement in experimental precision reduces the tolerance range and hence increases corroborability. In most psychological research, improved power of a statistical design leads to a prior probability approaching 1/2 of finding a significant difference in the theoretically predicted direction. Hence the corroboration yielded by "success" is very weak, and becomes weaker with increased precision. "Statistical significance" plays a logical role in psychology precisely the reverse of its role in physics. This problem is worsened by certain unhealthy tendencies prevalent among psychologists, such as a premium placed on experimental "cuteness" and a free reliance upon ad hoc explanations to avoid refutation.},
  file = {/Users/glupyan/Zotero/storage/M6QV6IHT/288135.html}
}

@article{moherPreferredReportingItems2015,
  title = {Preferred Reporting Items for Systematic Review and Meta-Analysis Protocols ({{PRISMA-P}}) 2015 Statement},
  author = {Moher, David and Shamseer, Larissa and Clarke, Mike and Ghersi, Davina and Liberati, Alessandro and Petticrew, Mark and Shekelle, Paul and Stewart, Lesley A. and {PRISMA-P Group}},
  date = {2015-01-01},
  journaltitle = {Systematic Reviews},
  shortjournal = {Syst Rev},
  volume = {4},
  number = {1},
  eprint = {25554246},
  eprinttype = {pubmed},
  pages = {1},
  issn = {2046-4053},
  doi = {10.1186/2046-4053-4-1},
  abstract = {Systematic reviews should build on a protocol that describes the rationale, hypothesis, and planned methods of the review; few reviews report whether a protocol exists. Detailed, well-described protocols can facilitate the understanding and appraisal of the review methods, as well as the detection of modifications to methods and selective reporting in completed reviews. We describe the development of a reporting guideline, the Preferred Reporting Items for Systematic reviews and Meta-Analyses for Protocols 2015 (PRISMA-P 2015). PRISMA-P consists of a 17-item checklist intended to facilitate the preparation and reporting of a robust protocol for the systematic review. Funders and those commissioning reviews might consider mandating the use of the checklist to facilitate the submission of relevant protocol information in funding applications. Similarly, peer reviewers and editors can use the guidance to gauge the completeness and transparency of a systematic review protocol submitted for publication in a journal or other medium.},
  langid = {english},
  pmcid = {PMC4320440},
  keywords = {Access to Information,Checklist,Evidence-Based Medicine,Guideline Adherence,Humans,Meta-Analysis as Topic,Publishing,Quality Control,Systematic Reviews as Topic},
  file = {/Users/glupyan/Zotero/storage/DITSX6DH/Moher et al. - 2015 - Preferred reporting items for systematic review and meta-analysis protocols (PRISMA-P) 2015 statemen.pdf}
}

@article{rouderPsychometricsIndividualDifferences2019,
  title = {A Psychometrics of Individual Differences in Experimental Tasks},
  author = {Rouder, Jeffrey N. and Haaf, Julia M.},
  date = {2019-04-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychon Bull Rev},
  volume = {26},
  number = {2},
  pages = {452--467},
  issn = {1531-5320},
  doi = {10.3758/s13423-018-1558-y},
  url = {https://doi.org/10.3758/s13423-018-1558-y},
  urldate = {2026-01-05},
  abstract = {In modern individual-difference studies, researchers often correlate performance on various tasks to uncover common latent processes. Yet, in some sense, the results have been disappointing as correlations among tasks that seemingly have processes in common are often low. A pressing question then is whether these attenuated correlations reflect statistical considerations, such as a lack of individual variability on tasks, or substantive considerations, such as that inhibition in different tasks is not a unified concept. One problem in addressing this question is that researchers aggregate performance across trials to tally individual-by-task scores. It is tempting to think that aggregation is fine and that everything comes out in the wash. But as shown here, this aggregation may greatly attenuate measures of effect size and correlation. We propose an alternative analysis of task performance that is based on accounting for trial-by-trial variability along with the covariation of individuals’ performance across tasks. The implementation is through common hierarchical models, and this treatment rescues classical concepts of effect size, reliability, and correlation for studying individual differences with experimental tasks. Using recent data from Hedge et al. Behavioral Research Methods, 50(3), 1166–1186, 2018 we show that there is Bayes-factor support for a lack of correlation between the Stroop and flanker task. This support for a lack of correlation indicates a psychologically relevant result—Stroop and flanker inhibition are seemingly unrelated, contradicting unified concepts of inhibition.},
  langid = {english},
  keywords = {Bayesian inference,Hierarchical models,Individual differences,Inhibition,Reliability},
  file = {/Users/glupyan/Zotero/storage/GLG2PNQQ/Rouder and Haaf - 2019 - A psychometrics of individual differences in experimental tasks.pdf}
}

@article{rozinWhatKindEmpirical2009,
  title = {What {{Kind}} of {{Empirical Research Should We Publish}}, {{Fund}}, and {{Reward}}?: {{A Different Perspective}}},
  shorttitle = {What {{Kind}} of {{Empirical Research Should We Publish}}, {{Fund}}, and {{Reward}}?},
  author = {Rozin, Paul},
  date = {2009-07},
  journaltitle = {Perspectives on Psychological Science: A Journal of the Association for Psychological Science},
  shortjournal = {Perspect Psychol Sci},
  volume = {4},
  number = {4},
  eprint = {26158991},
  eprinttype = {pubmed},
  pages = {435--439},
  issn = {1745-6916},
  doi = {10.1111/j.1745-6924.2009.01151.x},
  abstract = {When evaluating empirical papers for publication, grant proposals, or individual contributions (e.g., awarding tenure), the basic question one should ask is how much the contribution adds to understanding in psychology and not whether the contribution takes a particular form or represents one particular model of how to do empirical studies. Academic psychology has flourished with its mastery of the hypothesis-experiment model of science and its expertise in generating and eliminating alternative hypotheses and isolating causation. These accomplishments are a critical part of psychology, and they are well and appropriately taught by psychologists. However, they are only a part of science and should not comprise the almost exclusive criteria for evaluating research. In particular, discovery of fundamental phenomena, such as functional relations that apply to the real world and have generality, should have a higher priority in psychology. Such findings have been the basis for theoretical advances in other natural sciences.},
  langid = {english}
}

@article{sijtsmaUseMisuseVery2009,
  title = {On the {{Use}}, the {{Misuse}}, and the {{Very Limited Usefulness}} of~{{Cronbach}}’s {{Alpha}}},
  author = {Sijtsma, Klaas},
  date = {2009},
  journaltitle = {Psychometrika},
  shortjournal = {Psychometrika},
  volume = {74},
  number = {1},
  eprint = {20037639},
  eprinttype = {pubmed},
  pages = {107--120},
  issn = {0033-3123},
  doi = {10.1007/s11336-008-9101-0},
  url = {https://pmc.ncbi.nlm.nih.gov/articles/PMC2792363/},
  urldate = {2026-01-05},
  abstract = {This discussion paper argues that both the use of Cronbach’s alpha as a reliability estimate and as a measure of internal consistency suffer from major problems. First, alpha always has a value, which cannot be equal to the test score’s reliability given the interitem covariance matrix and the usual assumptions about measurement error. Second, in practice, alpha is used more often as a measure of the test’s internal consistency than as an estimate of reliability. However, it can be shown easily that alpha is unrelated to the internal structure of the test. It is further discussed that statistics based on a single test administration do not convey much information about the accuracy of individuals’ test performance. The paper ends with a list of conclusions about the usefulness of alpha.},
  pmcid = {PMC2792363},
  file = {/Users/glupyan/Zotero/storage/KIQLY28F/Sijtsma - 2009 - On the Use, the Misuse, and the Very Limited Usefulness of Cronbach’s Alpha.pdf}
}

@article{speelmanMostPsychologicalResearchers2024,
  title = {Most {{Psychological Researchers Assume Their Samples Are Ergodic}}: {{Evidence From}} a {{Year}} of {{Articles}} in {{Three Major Journals}}},
  shorttitle = {Most {{Psychological Researchers Assume Their Samples Are Ergodic}}},
  author = {Speelman, Craig P. and Parker, Laura and Rapley, Benjamin J. and McGann, Marek},
  editor = {Vargo, Elisabeth Julie},
  date = {2024-02-09},
  journaltitle = {Collabra: Psychology},
  shortjournal = {Collabra: Psychology},
  volume = {10},
  number = {1},
  pages = {92888},
  issn = {2474-7394},
  doi = {10.1525/collabra.92888},
  url = {https://doi.org/10.1525/collabra.92888},
  urldate = {2026-01-05},
  abstract = {Conventional statistics methods in most psychological research, such as null-hypothesis significance tests (NHSTs), use aggregated values (i.e., the sample means) of group behaviours to make inferences about individuals. Such inferences are possibly erroneous because groups of humans rarely, if ever, constitute an ergodic system. To assume ergodicity without checking is to commit the ‘ergodic fallacy’. The aim of the current study was to examine the prevalence of this error in contemporary psychological research. We analysed three highly cited ‘Q1’ journals in the fields of clinical, educational and cognitive psychology for statements that indicated this error. As hypothesized, the ergodic fallacy was found in the vast majority of the papers investigated here. We also hypothesised that the prevalence of this error would be highest in cognitive psychology papers because this field typically assesses theoretical claims about universal cognitive mechanisms, whereas clinical and educational psychology are more concerned with empirically supported interventions. This hypothesis was also supported by our results. Nonetheless, the prevalence of the ergodic fallacy was still high in all fields. Implications are discussed with respect to the reporting of research findings and the validity of theories in psychology.},
  file = {/Users/glupyan/Zotero/storage/AK87HTRM/Speelman et al. - 2024 - Most Psychological Researchers Assume Their Samples Are Ergodic Evidence From a Year of Articles in.pdf;/Users/glupyan/Zotero/storage/TKQDJUS9/collabra.html}
}

@article{speelmanStatementsPervasivenessBehavior2020,
  title = {Statements {{About}} the {{Pervasiveness}} of {{Behavior Require Data About}} the {{Pervasiveness}} of {{Behavior}}},
  author = {Speelman, Craig P. and McGann, Marek},
  date = {2020-11-19},
  journaltitle = {Frontiers in Psychology},
  shortjournal = {Front. Psychol.},
  volume = {11},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2020.594675},
  url = {https://www.frontiersin.org/journals/psychology/articles/10.3389/fpsyg.2020.594675/full},
  urldate = {2026-01-05},
  abstract = {Despite recent close attention to issues related to the reliability of psychological research (e.g., the replication crisis), issues of the validity of this research have not been considered to the same extent. This paper highlights an issue that calls into question the validity of the common research practice of studying samples of individuals, and using sample-based statistics to infer generalisations that are applied not only to the parent population, but to individuals. The lack of ergodicity in human data means that such generalizations are not justified. This problem is illustrated with respect to two common scenarios in psychological research that raise questions for the sorts of theories that are typically proposed to explain human behaviour and cognition. The paper presents a method of data analysis that requires closer attention to the range of behaviours exhibited by individuals in our research to determine the pervasiveness of effects observed in sample data. Such an approach to data analysis will produce results that are more in tune with the types of generalisations typical in reports of psychological research than mainstream analysis methods.},
  langid = {english},
  keywords = {ergodicity,Individuality,pervasiveness,scientific practice,validity},
  file = {/Users/glupyan/Zotero/storage/2NB6WAM6/Speelman and McGann - 2020 - Statements About the Pervasiveness of Behavior Require Data About the Pervasiveness of Behavior.pdf}
}

@article{telzerMethodologicalConsiderationsDevelopmental2018,
  title = {Methodological Considerations for Developmental Longitudinal {{fMRI}} Research},
  author = {Telzer, Eva H. and McCormick, Ethan M. and Peters, Sabine and Cosme, Danielle and Pfeifer, Jennifer H. and family=Duijvenvoorde, given=Anna C. K., prefix=van, useprefix=true},
  date = {2018-10-01},
  journaltitle = {Developmental Cognitive Neuroscience},
  shortjournal = {Developmental Cognitive Neuroscience},
  series = {Methodological {{Challenges}} in {{Developmental Neuroimaging}}: {{Contemporary Approaches}} and {{Solutions}}},
  volume = {33},
  pages = {149--160},
  issn = {1878-9293},
  doi = {10.1016/j.dcn.2018.02.004},
  url = {https://www.sciencedirect.com/science/article/pii/S1878929317301950},
  urldate = {2026-01-05},
  abstract = {There has been a large spike in longitudinal fMRI studies in recent years, and so it is essential that researchers carefully assess the limitations and challenges afforded by longitudinal designs. In this article, we provide an overview of important considerations for longitudinal fMRI research in developmental samples, including task design, sampling strategies, and group-level analyses. We first discuss considerations for task designs, weighing the pros and cons of many commonly used tasks, as well as outlining how the tasks may be impacted by repeated exposure. Secondly, we review the types of group-level analyses that can be conducted on longitudinal fMRI data, analyses which must account for repeated measures. Finally, we review and critique recent longitudinal studies that have emerged in the past few years.},
  keywords = {Development,Longitudinal fMRI,Methods},
  file = {/Users/glupyan/Zotero/storage/JQNH96B7/Telzer et al. - 2018 - Methodological considerations for developmental longitudinal fMRI research.pdf;/Users/glupyan/Zotero/storage/WHMZ6H5N/S1878929317301950.html}
}

@article{wagenmakersPracticalSolutionPervasive2007,
  title = {A Practical Solution to the Pervasive Problems Ofp Values},
  author = {Wagenmakers, Eric-Jan},
  date = {2007-10-01},
  journaltitle = {Psychonomic Bulletin \& Review},
  shortjournal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1531-5320},
  doi = {10.3758/BF03194105},
  url = {https://doi.org/10.3758/BF03194105},
  urldate = {2026-01-05},
  abstract = {In the field of psychology, the practice ofp value null-hypothesis testing is as widespread as ever. Despite this popularity, or perhaps because of it, most psychologists are not aware of the statistical peculiarities of thep value procedure. In particular,p values are based on data that were never observed, and these hypothetical data are themselves influenced by subjective intentions. Moreover,p values do not quantify statistical evidence. This article reviews thesep value problems and illustrates each problem with concrete examples. The three problems are familiar to statisticians but may be new to psychologists. A practical solution to thesep value problems is to adopt a model selection perspective and use the Bayesian information criterion (BIC) for statistical inference (Raftery, 1995). The BIC provides an approximation to a Bayesian hypothesis test, does not require the specification of priors, and can be easily calculated from SPSS output.},
  langid = {english},
  keywords = {Bayesian Information Criterion,Null Hypothesis,Posterior Probability,Prior Distribution,Statistical Inference},
  file = {/Users/glupyan/Zotero/storage/5ITZRG7G/Wagenmakers - 2007 - A practical solution to the pervasive problems ofp values.pdf}
}

@article{westfallStatisticallyControllingConfounding2016,
  title = {Statistically {{Controlling}} for {{Confounding Constructs Is Harder}} than {{You Think}}},
  author = {Westfall, Jacob and Yarkoni, Tal},
  date = {2016-03-31},
  journaltitle = {PLOS ONE},
  shortjournal = {PLOS ONE},
  volume = {11},
  number = {3},
  pages = {e0152719},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0152719},
  url = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0152719},
  urldate = {2026-01-05},
  abstract = {Social scientists often seek to demonstrate that a construct has incremental validity over and above other related constructs. However, these claims are typically supported by measurement-level models that fail to consider the effects of measurement (un)reliability. We use intuitive examples, Monte Carlo simulations, and a novel analytical framework to demonstrate that common strategies for establishing incremental construct validity using multiple regression analysis exhibit extremely high Type I error rates under parameter regimes common in many psychological domains. Counterintuitively, we find that error rates are highest—in some cases approaching 100\%—when sample sizes are large and reliability is moderate. Our findings suggest that a potentially large proportion of incremental validity claims made in the literature are spurious. We present a web application (http://jakewestfall.org/ivy/) that readers can use to explore the statistical properties of these and other incremental validity arguments. We conclude by reviewing SEM-based statistical approaches that appropriately control the Type I error rate when attempting to establish incremental validity.},
  langid = {english},
  keywords = {Behavior,Emotions,Forecasting,Personality,Research errors,Social sciences,Swimming,Test statistics},
  file = {/Users/glupyan/Zotero/storage/6N4YZRJE/Westfall and Yarkoni - 2016 - Statistically Controlling for Confounding Constructs Is Harder than You Think.pdf}
}

@article{westwoodPotentialExistentialThreat2025,
  title = {The Potential Existential Threat of Large Language Models to Online Survey Research},
  author = {Westwood, Sean J.},
  date = {2025-11-25},
  journaltitle = {Proceedings of the National Academy of Sciences},
  volume = {122},
  number = {47},
  pages = {e2518075122},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2518075122},
  url = {https://www.pnas.org/doi/10.1073/pnas.2518075122},
  urldate = {2026-01-05},
  abstract = {The advancement of large language models poses a severe, potentially existential threat to online survey research, a fundamental tool for data collection across the sciences. This work demonstrates that the foundational assumption of survey research—that a coherent response is a human response—is no longer tenable. I designed and tested an autonomous synthetic respondent capable of producing survey data that possesses the coherence and plausibility of human responses. This agent successfully evades a comprehensive suite of data quality checks, including instruction-following tasks, logic puzzles, and “reverse shibboleth” questions designed to detect nonhuman actors, achieving a 99.8\% pass rate on 6,000 trials of standard attention checks. The synthetic respondent generates internally consistent responses by maintaining a coherent demographic persona and a memory of its prior answers, producing plausible data on psychometric scales, vignette comprehension tasks, and complex socioeconomic trade-offs. Furthermore, its open-ended text responses are linguistically sophisticated and stylistically calibrated to the level of education of its assigned persona. Critically, the agent can be instructed to maliciously alter polling outcomes, demonstrating an overt vector for information warfare. More subtly, it can also infer a researcher’s latent hypotheses and produce data that artificially confirms them. These findings reveal a critical vulnerability in our data infrastructure, rendering most current detection methods obsolete and posing a potential existential threat to unsupervised online research. The scientific community must urgently develop new data validation standards and reconsider its reliance on nonprobability, low-barrier online data collection methods.},
  file = {/Users/glupyan/Zotero/storage/WHQ8JJB8/Westwood - 2025 - The potential existential threat of large language models to online survey research.pdf}
}

@article{wilkinsonFAIRGuidingPrinciples2016,
  title = {The {{FAIR Guiding Principles}} for Scientific Data Management and Stewardship},
  author = {Wilkinson, Mark D. and Dumontier, Michel and family=Aalbersberg, given=IJsbrand Jan, given-i={{IJ}}J and Appleton, Gabrielle and Axton, Myles and Baak, Arie and Blomberg, Niklas and Boiten, Jan-Willem and family=Silva Santos, given=Luiz Bonino, prefix=da, useprefix=true and Bourne, Philip E. and Bouwman, Jildau and Brookes, Anthony J. and Clark, Tim and Crosas, Mercè and Dillo, Ingrid and Dumon, Olivier and Edmunds, Scott and Evelo, Chris T. and Finkers, Richard and Gonzalez-Beltran, Alejandra and Gray, Alasdair J. G. and Groth, Paul and Goble, Carole and Grethe, Jeffrey S. and Heringa, Jaap and family=Hoen, given=Peter A. C., prefix=’t, useprefix=true and Hooft, Rob and Kuhn, Tobias and Kok, Ruben and Kok, Joost and Lusher, Scott J. and Martone, Maryann E. and Mons, Albert and Packer, Abel L. and Persson, Bengt and Rocca-Serra, Philippe and Roos, Marco and family=Schaik, given=Rene, prefix=van, useprefix=true and Sansone, Susanna-Assunta and Schultes, Erik and Sengstag, Thierry and Slater, Ted and Strawn, George and Swertz, Morris A. and Thompson, Mark and family=Lei, given=Johan, prefix=van der, useprefix=true and family=Mulligen, given=Erik, prefix=van, useprefix=true and Velterop, Jan and Waagmeester, Andra and Wittenburg, Peter and Wolstencroft, Katherine and Zhao, Jun and Mons, Barend},
  date = {2016-03-15},
  journaltitle = {Scientific Data},
  shortjournal = {Sci Data},
  volume = {3},
  number = {1},
  pages = {160018},
  publisher = {Nature Publishing Group},
  issn = {2052-4463},
  doi = {10.1038/sdata.2016.18},
  url = {https://www.nature.com/articles/sdata201618},
  urldate = {2026-01-05},
  abstract = {There is an urgent need to improve the infrastructure supporting the reuse of scholarly data. A diverse set of stakeholders—representing academia, industry, funding agencies, and scholarly publishers—have come together to design and jointly endorse a concise and measureable set of principles that we refer to as the FAIR Data Principles. The intent is that these may act as a guideline for those wishing to enhance the reusability of their data holdings. Distinct from peer initiatives that focus on the human scholar, the FAIR Principles put specific emphasis on enhancing the ability of machines to automatically find and use the data, in addition to supporting its reuse by individuals. This Comment is the first formal publication of the FAIR Principles, and includes the rationale behind them, and some exemplar implementations in the community.},
  langid = {english},
  keywords = {Publication characteristics,Research data},
  file = {/Users/glupyan/Zotero/storage/VYZDIBJ5/Wilkinson et al. - 2016 - The FAIR Guiding Principles for scientific data management and stewardship.pdf}
}

@article{yarkoniGeneralizabilityCrisis2020,
  title = {The Generalizability Crisis},
  author = {Yarkoni, Tal},
  date = {2020-12-21},
  journaltitle = {The Behavioral and Brain Sciences},
  shortjournal = {Behav Brain Sci},
  volume = {45},
  eprint = {33342451},
  eprinttype = {pubmed},
  pages = {e1},
  issn = {1469-1825},
  doi = {10.1017/S0140525X20001685},
  abstract = {Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned - that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology - the linear mixed model - I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the "random effect" formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
  langid = {english},
  pmcid = {PMC10681374},
  keywords = {Generalization,Humans,inference,Intention,philosophy of science,psychology,Psychology,random effects,statistics},
  file = {/Users/glupyan/Zotero/storage/KWY68JUW/Yarkoni - 2020 - The generalizability crisis.pdf}
}

@article{zorowitzImprovingReliabilityCognitive2023,
  title = {Improving the {{Reliability}} of {{Cognitive Task Measures}}: {{A Narrative Review}}},
  shorttitle = {Improving the {{Reliability}} of {{Cognitive Task Measures}}},
  author = {Zorowitz, Samuel and Niv, Yael},
  date = {2023-08},
  journaltitle = {Biological Psychiatry. Cognitive Neuroscience and Neuroimaging},
  shortjournal = {Biol Psychiatry Cogn Neurosci Neuroimaging},
  volume = {8},
  number = {8},
  eprint = {36842498},
  eprinttype = {pubmed},
  pages = {789--797},
  issn = {2451-9030},
  doi = {10.1016/j.bpsc.2023.02.004},
  abstract = {Cognitive tasks are capable of providing researchers with crucial insights into the relationship between cognitive processing and psychiatric phenomena. However, many recent studies have found that task measures exhibit poor reliability, which hampers their usefulness for individual differences research. Here, we provide a narrative review of approaches to improve the reliability of cognitive task measures. Specifically, we introduce a taxonomy of experiment design and analysis strategies for improving task reliability. Where appropriate, we highlight studies that are exemplary for improving the reliability of specific task measures. We hope that this article can serve as a helpful guide for experimenters who wish to design a new task, or improve an existing one, to achieve sufficient reliability for use in individual differences research.},
  langid = {english},
  pmcid = {PMC10440239},
  keywords = {Behavioral tasks,Cognition,Cognitive functions,Computational psychiatry,Humans,Individual differences,Individuality,Psychometrics,Reliability,Reproducibility of Results},
  file = {/Users/glupyan/Zotero/storage/47H6UH8Z/Zorowitz and Niv - 2023 - Improving the Reliability of Cognitive Task Measures A Narrative Review.pdf}
}
